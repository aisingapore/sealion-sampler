# For local model
LOCAL_MODEL="aisingapore/sea-lion-3b"
# For Ollama Server (/chat or /generate endpoints accepted)
OLL_API_URL=http://localhost:11434/api/chat
OLL_API_MODEL=aisingapore/llama3-8b-cpt-sea-lionv2-instruct
# For Inference Server REST API
TGI_API_URL=https://tgi-api.com/dev/chat/completions
TGI_API_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
TGI_MODEL1=aisingapore/sea-lion-7b-instruct
TGI_MODEL2=meta/llama-2-7b-instruct